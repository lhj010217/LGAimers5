{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8504dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4dd17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2292aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "=====================데이터 전처리과정 개요=====================\n",
    "1. 초기 train, test 데이터 로드\n",
    "2. 이상치 처리, 결측치 처리, 단일 값으로 채워진 컬럼 삭제\n",
    "3. 원핫 인코딩 수행\n",
    "4. 데이터 스케일링 수행\n",
    "5. 전처리된 데이터 저장(학습 시 전처리 과정을 생략하기 위함)\n",
    "===========================================================\n",
    "'''\n",
    "\n",
    "# 초기 데이터 로드\n",
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 110\n",
    "train_data_ori = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n",
    "test_data_ori = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))\n",
    "train_data_ori\n",
    "\n",
    "# 이상치(OK값) 처리, 결측치 처리, 하나의 값으로만 채워진 컬럼 삭제\n",
    "def preprocess_column(df):\n",
    "    df = df.replace('OK', 0)\n",
    "    df_dropped = df.dropna(axis=1, how='all')\n",
    "    df_dropped = df_dropped.loc[:, df.apply(pd.Series.nunique) > 1]\n",
    "    df_dropped = df_dropped.fillna(0)\n",
    "    return df_dropped\n",
    "\n",
    "train_data_dropped = preprocess_column(train_data_ori)\n",
    "test_data_dropped = preprocess_column(test_data_ori)\n",
    "\n",
    "# test_data의 제품 코드값 제외\n",
    "test_data_dropped = test_data_dropped.drop(test_data_dropped.columns[0], axis=1)\n",
    "\n",
    "# 데이터 전처리 수행\n",
    "def preprocess_data(train_df, test_df, n_components=3000):\n",
    "    \n",
    "    # 마지막 열 추출(target 컬럼)\n",
    "    last_column_train = train_df.iloc[:, -1]\n",
    "    data_train_without_last_column = train_df.iloc[:, :-1]\n",
    "    \n",
    "    data_test_without_last_column = test_df\n",
    "\n",
    "    # 두 데이터셋을 결합하여 원핫 인코딩 수행\n",
    "    # 원핫인코딩의 결과로 생긴 컬럼의 이름이 다른 경우를 방지\n",
    "    combined_data = pd.concat([data_train_without_last_column, data_test_without_last_column], keys=['train', 'test'])\n",
    "    combined_data_encoded = pd.get_dummies(combined_data)\n",
    "    combined_data_encoded.fillna(0, inplace=True)\n",
    "    \n",
    "    # 다시 train/test 데이터셋으로 분리\n",
    "    train_encoded = combined_data_encoded.xs('train')\n",
    "    test_encoded = combined_data_encoded.xs('test')\n",
    "    \n",
    "    # 데이터 스케일링\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_encoded)\n",
    "    test_scaled = scaler.transform(test_encoded)\n",
    "\n",
    "    # 스케일링된 데이터를 데이터프레임으로 변환\n",
    "    train_scaled_df = pd.DataFrame(train_scaled, columns=train_encoded.columns)\n",
    "    test_scaled_df = pd.DataFrame(test_scaled, columns=test_encoded.columns)\n",
    "    \n",
    "    # 마지막 열 다시 데이터프레임에 붙이기\n",
    "    train_processed = pd.concat([train_scaled_df, last_column_train.reset_index(drop=True)], axis=1)\n",
    "    test_processed = test_scaled_df \n",
    "    \n",
    "    return train_processed, test_processed\n",
    "\n",
    "# 데이터 전처리 이후 저장\n",
    "train_data, test_data = preprocess_data(train_data_dropped, test_data_dropped)\n",
    "train_data.to_csv(\"data/train_processed\", index=False)\n",
    "test_data.to_csv(\"data/test_processed\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737e5c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/1992689504.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean Head Coordinate Z Axis'] = (df['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'] +\n",
      "/tmp/ipykernel_55/1992689504.py:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 1_2'] = np.where(df['2nd Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 1_3'] = np.where(df['3rd Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 2_1'] = np.where(df['1st Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 2_3'] = np.where(df['3rd Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 3_1'] = np.where(df['1st Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 3_2'] = np.where(df['2nd Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['3rd Pressure Collect Result_AutoClave_Binary'] = df['3rd Pressure Collect Result_AutoClave'].apply(lambda x: 1 if x > 0 else 0)\n",
      "/tmp/ipykernel_55/1992689504.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean Head Coordinate Z Axis'] = (df['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'] +\n",
      "/tmp/ipykernel_55/1992689504.py:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 1_2'] = np.where(df['2nd Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 1_3'] = np.where(df['3rd Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 2_1'] = np.where(df['1st Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 2_3'] = np.where(df['3rd Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 3_1'] = np.where(df['1st Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pressure Ratio 3_2'] = np.where(df['2nd Pressure Collect Result_AutoClave'] != 0,\n",
      "/tmp/ipykernel_55/1992689504.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['3rd Pressure Collect Result_AutoClave_Binary'] = df['3rd Pressure Collect Result_AutoClave'].apply(lambda x: 1 if x > 0 else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.064374\n",
      "0:\tlearn: 0.6882910\ttest: 0.6880696\tbest: 0.6880696 (0)\ttotal: 9.64ms\tremaining: 11.6s\n",
      "100:\tlearn: 0.6052074\ttest: 0.6077812\tbest: 0.6077812 (100)\ttotal: 793ms\tremaining: 8.63s\n",
      "200:\tlearn: 0.5628127\ttest: 0.5720906\tbest: 0.5720906 (200)\ttotal: 1.58s\tremaining: 7.84s\n",
      "300:\tlearn: 0.5201492\ttest: 0.5367433\tbest: 0.5367433 (300)\ttotal: 2.36s\tremaining: 7.06s\n",
      "400:\tlearn: 0.4853823\ttest: 0.5082752\tbest: 0.5082752 (400)\ttotal: 3.15s\tremaining: 6.27s\n",
      "500:\tlearn: 0.4546777\ttest: 0.4836594\tbest: 0.4836594 (500)\ttotal: 3.92s\tremaining: 5.46s\n",
      "600:\tlearn: 0.4288529\ttest: 0.4625670\tbest: 0.4625670 (600)\ttotal: 4.69s\tremaining: 4.67s\n",
      "700:\tlearn: 0.4033405\ttest: 0.4416593\tbest: 0.4416593 (700)\ttotal: 5.49s\tremaining: 3.91s\n",
      "800:\tlearn: 0.3815444\ttest: 0.4243053\tbest: 0.4243053 (800)\ttotal: 6.26s\tremaining: 3.12s\n",
      "900:\tlearn: 0.3623726\ttest: 0.4095535\tbest: 0.4095535 (900)\ttotal: 7.04s\tremaining: 2.33s\n",
      "1000:\tlearn: 0.3437514\ttest: 0.3949579\tbest: 0.3949579 (1000)\ttotal: 7.83s\tremaining: 1.56s\n",
      "1100:\tlearn: 0.3273357\ttest: 0.3820668\tbest: 0.3820668 (1100)\ttotal: 8.62s\tremaining: 775ms\n",
      "1199:\tlearn: 0.3116694\ttest: 0.3689906\tbest: 0.3689906 (1199)\ttotal: 9.39s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.3689906352\n",
      "bestIteration = 1199\n",
      "\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "=======================머신러닝 과정 개요=======================\n",
    "1. 전처리된 데이터(train_processed) 로드\n",
    "2. Normal/AbNormal간의 데이터 불균형 해소를 위해 \n",
    "   Normal 데이터는 15000개로 언더샘플링,\n",
    "   AbNormal 데이터는 12000개로 오버샘플링. \n",
    "3. 두 데이터를 병합하여 df_concat으로 생성\n",
    "4. train_test_split을 이용해 90%의 train 데이터와 10%의 validation 데이터로 분할\n",
    "5. feature importance를 얻기 위한 1차 학습 진행. Catboost Classifier를 이용\n",
    "6. 학습 시간과 복잡도를 줄이기 위해 중요도가 0.01이 넘는 feature만 채택.\n",
    "7. 정확도를 높이기 위한 feature engineering 진행\n",
    "   1,2,3번째 단계의 상호압력 비율, 3번째 단계의 압력에 대한 바이너리 feature 생성\n",
    "8. 6에서 채택된 feature와 7에서 생성한 feature를 병합하여 최종 train_x, val_x 생성\n",
    "9. test data에 대해서도 동일한 feature engineering 진행.\n",
    "10. 최종적으로 Catboost Classifier를 이용하여 학습 및 test 데이터에 대한 예측 수행\n",
    "11. submission.csv 파일 생성\n",
    "===========================================================\n",
    "'''\n",
    "\n",
    "# 전처리된 데이터 불러오기\n",
    "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train_processed\"))\n",
    "\n",
    "# \"Normal\"과 \"AbNormal\" 샘플링\n",
    "df_normal = train_data[train_data[\"target\"] == \"Normal\"]\n",
    "df_normal = df_normal.sample(n=15000, random_state=RANDOM_STATE)\n",
    "\n",
    "df_abnormal = train_data[train_data[\"target\"] == \"AbNormal\"]\n",
    "df_abnormal = resample(df_abnormal, replace=True, n_samples=12000, random_state=RANDOM_STATE)\n",
    "\n",
    "# 데이터 병합\n",
    "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
    "\n",
    "# train-validation split\n",
    "df_train, df_val = train_test_split(\n",
    "    df_concat,\n",
    "    test_size=0.1,\n",
    "    stratify=df_concat[\"target\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# CatBoost 모델 초기화\n",
    "model = CatBoostClassifier(random_state=RANDOM_STATE, verbose=0)\n",
    "\n",
    "# 피처 리스트 생성 및 데이터 타입 변환\n",
    "features = []\n",
    "for col in df_train.columns:\n",
    "    try:\n",
    "        df_train[col] = df_train[col].astype(float)\n",
    "        df_val[col] = df_val[col].astype(float)\n",
    "        features.append(col)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "train_x = df_train[features]\n",
    "train_y = df_train[\"target\"]\n",
    "\n",
    "val_x = df_val[features]\n",
    "val_y = df_val[\"target\"]\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_x, train_y, eval_set=(val_x, val_y))\n",
    "\n",
    "# 피처 중요도 추출\n",
    "feature_importances = model.get_feature_importance(Pool(train_x, label=train_y))\n",
    "feature_importance_dict = dict(zip(features, feature_importances))\n",
    "\n",
    "# 중요도가 0.01을 넘는 피처 필터링\n",
    "selected_features = [feat for feat, importance in feature_importance_dict.items() if importance > 0.01]\n",
    "  \n",
    "def create_engineered_features(df):\n",
    "    df['Mean Head Coordinate Z Axis'] = (df['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'] + \n",
    "                                         df['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'] + \n",
    "                                         df['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1']) / 3\n",
    "    df['Pressure Ratio 1_2'] = np.where(df['2nd Pressure Collect Result_AutoClave'] != 0,\n",
    "                                        df['1st Pressure Collect Result_AutoClave'] / df['2nd Pressure Collect Result_AutoClave'],\n",
    "                                        np.nan)    \n",
    "    df['Pressure Ratio 1_3'] = np.where(df['3rd Pressure Collect Result_AutoClave'] != 0,\n",
    "                                        df['1st Pressure Collect Result_AutoClave'] / df['3rd Pressure Collect Result_AutoClave'],\n",
    "                                        np.nan)    \n",
    "    df['Pressure Ratio 2_1'] = np.where(df['1st Pressure Collect Result_AutoClave'] != 0,\n",
    "                                        df['2nd Pressure Collect Result_AutoClave'] / df['1st Pressure Collect Result_AutoClave'],\n",
    "                                        np.nan)    \n",
    "    df['Pressure Ratio 2_3'] = np.where(df['3rd Pressure Collect Result_AutoClave'] != 0,\n",
    "                                        df['2nd Pressure Collect Result_AutoClave'] / df['3rd Pressure Collect Result_AutoClave'],\n",
    "                                        np.nan)   \n",
    "    df['Pressure Ratio 3_1'] = np.where(df['1st Pressure Collect Result_AutoClave'] != 0,\n",
    "                                        df['3rd Pressure Collect Result_AutoClave'] / df['1st Pressure Collect Result_AutoClave'],\n",
    "                                        np.nan)   \n",
    "    df['Pressure Ratio 3_2'] = np.where(df['2nd Pressure Collect Result_AutoClave'] != 0,\n",
    "                                        df['3rd Pressure Collect Result_AutoClave'] / df['2nd Pressure Collect Result_AutoClave'],\n",
    "                                        np.nan)  \n",
    "    df['3rd Pressure Collect Result_AutoClave_Binary'] = df['3rd Pressure Collect Result_AutoClave'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 추가된 피처 목록\n",
    "additional_features = [\n",
    "    'Mean Head Coordinate Z Axis',\n",
    "    'Pressure Ratio 1_2',\n",
    "    'Pressure Ratio 1_3',\n",
    "    'Pressure Ratio 2_1',\n",
    "    'Pressure Ratio 2_3',\n",
    "    'Pressure Ratio 3_1',\n",
    "    'Pressure Ratio 3_2',\n",
    "    '3rd Pressure Collect Result_AutoClave_Binary',\n",
    "]\n",
    "\n",
    "\n",
    "# 훈련 데이터에 피처 엔지니어링 적용\n",
    "train_x = create_engineered_features(train_x)\n",
    "\n",
    "# 검증 데이터에 피처 엔지니어링 적용\n",
    "val_x = create_engineered_features(val_x)\n",
    "    \n",
    "# 테스트 데이터에 피처 엔지니어링 적용\n",
    "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test_processed\"))\n",
    "df_test_x = create_engineered_features(test_data)\n",
    "\n",
    "# 선택된 피처와 엔지니어링된 피처를 포함한 데이터 준비\n",
    "train_x_final = train_x[selected_features + additional_features]\n",
    "val_x_final = val_x[selected_features + additional_features]\n",
    "df_test_x_final = df_test_x[selected_features + additional_features]\n",
    "\n",
    "# 모델 훈련\n",
    "model_filtered = CatBoostClassifier(random_state=RANDOM_STATE, verbose=100, iterations=1200)\n",
    "model_filtered.fit(train_x_final, \n",
    "                   train_y, \n",
    "                   eval_set=(val_x_final, val_y),\n",
    "                  )\n",
    "\n",
    "# 예측 수행\n",
    "test_pred = model_filtered.predict(df_test_x_final)\n",
    "\n",
    "# 제출 파일 생성\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = test_pred\n",
    "\n",
    "df_sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"End\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1a6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3fce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
